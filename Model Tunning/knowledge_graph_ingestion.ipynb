{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626d8f44",
   "metadata": {},
   "source": [
    "### Set up and environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j Credentials\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Google API Key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "\n",
    "DATA_DIR = \"./Research Data set\"\n",
    "TEXT_FOLDER = os.path.join(DATA_DIR, \"text\")\n",
    "TABLES_FOLDER = os.path.join(DATA_DIR, \"tables_data\")\n",
    "IMAGES_FILE = os.path.join(DATA_DIR, \"images_data.json\")\n",
    "CSV_FILE = os.path.join(DATA_DIR, \"SB_publication_PMC.csv\")\n",
    "\n",
    "print(\"Setup complete. Ensure your .env file and data paths are correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ca81f",
   "metadata": {},
   "source": [
    "### Initializing connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3625494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Connections ---\n",
    "\n",
    "# Initialize the Neo4j graph connection\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# Initialize the LLM for graph transformation (using a powerful model is key)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-latest\", temperature=0)\n",
    "\n",
    "# Initialize the multimodal LLM for image analysis\n",
    "llm_vision = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-latest\", temperature=0)\n",
    "\n",
    "\n",
    "# --- Define Graph Schema ---\n",
    "# This is the prompt that instructs the LLMGraphTransformer\n",
    "# It's based on our stakeholder-aware schema discussion.\n",
    "\n",
    "graph_creation_prompt = \"\"\"\n",
    "You are a brilliant NASA biologist and data scientist. Your task is to extract a knowledge graph from the provided research paper text.\n",
    "\n",
    "1.  **Nodes**: Identify all relevant entities and classify them into one of the following categories:\n",
    "    * `Paper`: The research paper itself.\n",
    "    * `BioEntity`: Biological components like Genes, Proteins, Cell Types, Molecules.\n",
    "    * `Concept`: Abstract ideas or processes like \"Bone Loss\", \"Oxidative Stress\", Health Risks, or Diseases.\n",
    "    * `Stressor`: Environmental factors unique to space like \"Microgravity\", \"Galactic Cosmic Rays\".\n",
    "    * `Organism`: The subject of the study (e.g., \"Mus musculus\", \"Homo sapiens\").\n",
    "    * `MissionContext`: Missions, hardware, or facilities like \"ISS Expedition 41\", \"Rodent Research-1\".\n",
    "    * `Application`: Potential real-world benefits like \"Osteoporosis Treatment\", \"Cancer Therapy\".\n",
    "    * `Institution`: Organizations involved.\n",
    "\n",
    "2.  **Relationships**: Identify the relationships between these entities. Use the following relationship types:\n",
    "    * `AFFECTS`: The primary relationship for scientific findings. **Crucially, add an `effect` property** to describe the nature of the effect (e.g., 'upregulates', 'inhibits', 'causes', 'correlates_with'). Also add an `evidence` property with the text snippet that supports the finding.\n",
    "    * `INVESTIGATES`: Connects a `Paper` to what it studies.\n",
    "    * `STUDIED_IN`: Connects a finding or entity to the `Organism`.\n",
    "    * `PART_OF`: Links research to a `MissionContext`.\n",
    "    * `HAS_POTENTIAL`: Links a finding to an `Application`.\n",
    "    * `AFFILIATED_WITH`: Connects a `Paper` to an `Institution`.\n",
    "\n",
    "Provide the output as a list of graph nodes and a list of graph relationships. Do not add any nodes or relationships that are not explicitly mentioned in the text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize the LLMGraphTransformer\n",
    "transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    prompt=graph_creation_prompt\n",
    ")\n",
    "\n",
    "print(\"Graph schema defined and connections initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f62122",
   "metadata": {},
   "source": [
    "### Unified context processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_text_chunk(text_chunk, paper_node):\n",
    "    \"\"\"Processes a plain text chunk and adds it to the graph.\"\"\"\n",
    "    if not text_chunk.strip():\n",
    "        return\n",
    "    print(f\"  Processing text chunk of {len(text_chunk)} chars...\")\n",
    "    document = Document(page_content=text_chunk)\n",
    "    graph_documents = transformer.convert_to_graph_documents([document])\n",
    "    graph.add_graph_documents(graph_documents, base=paper_node)\n",
    "\n",
    "def process_table(table_id, context_text, paper_node):\n",
    "    \"\"\"Processes a table with its context, adds to graph, and creates VisualEvidence.\"\"\"\n",
    "    print(f\"  Processing table: {table_id}...\")\n",
    "    table_html_path = os.path.join(TABLES_FOLDER, f\"{table_id}.html\")\n",
    "    if not os.path.exists(table_html_path):\n",
    "        print(f\"    [WARN] Table HTML file not found: {table_id}.html\")\n",
    "        return\n",
    "\n",
    "    with open(table_html_path, 'r') as f:\n",
    "        table_html = f.read()\n",
    "\n",
    "    # Create a rich prompt for the LLM\n",
    "    table_prompt = f\"\"\"\n",
    "    The following text is a snippet from a research paper that references a table:\n",
    "    ---\n",
    "    CONTEXT: \"{context_text}\"\n",
    "    ---\n",
    "    The full data for {table_id} is provided here in HTML format:\n",
    "    ---\n",
    "    TABLE DATA: \"{table_html}\"\n",
    "    ---\n",
    "    Based on BOTH the text context and the table data, extract all relevant scientific entities and their relationships.\n",
    "    \"\"\"\n",
    "    document = Document(page_content=table_prompt)\n",
    "    graph_documents = transformer.convert_to_graph_documents([document])\n",
    "    graph.add_graph_documents(graph_documents, base=paper_node)\n",
    "\n",
    "    # Create the VisualEvidence node for the table\n",
    "    # This Cypher query creates the node and links it to the concepts found in the context\n",
    "    for doc in graph_documents:\n",
    "        for node in doc.nodes:\n",
    "            # We only want to link to non-Paper nodes\n",
    "            if node.label != 'Paper':\n",
    "                cypher_query = \"\"\"\n",
    "                MERGE (p:Paper {id: $paper_id})\n",
    "                MERGE (v:VisualEvidence {id: $table_id, type: 'Table', content: $content, caption: $caption})\n",
    "                MERGE (c:%s {id: $concept_id})\n",
    "                MERGE (v)-[:ILLUSTRATES]->(c)\n",
    "                MERGE (p)-[:HAS_EVIDENCE]->(v) // Link the paper to the visual\n",
    "                \"\"\" % node.label\n",
    "                graph.query(cypher_query, params={\n",
    "                    \"paper_id\": paper_node['properties']['id'],\n",
    "                    \"table_id\": table_id,\n",
    "                    \"content\": f\"{table_id}.html\",\n",
    "                    \"caption\": context_text,\n",
    "                    \"concept_id\": node.id\n",
    "                })\n",
    "\n",
    "def process_image(image_id, context_text, paper_node, image_url_map):\n",
    "    \"\"\"\n",
    "    Processes an image with its context, adds to graph, and creates VisualEvidence.\n",
    "    This is the actual implementation for the multimodal call.\n",
    "    \"\"\"\n",
    "    print(f\"  Processing image: {image_id}...\")\n",
    "    image_url = image_url_map.get(image_id)\n",
    "    if not image_url:\n",
    "        print(f\"    [WARN] Image URL not found for: {image_id}\")\n",
    "        return\n",
    "\n",
    "    # Construct the multimodal message for Gemini\n",
    "    vision_prompt_text = f\"\"\"\n",
    "    Analyze the following scientific image in the context of its caption from a research paper.\n",
    "    ---\n",
    "    CAPTION CONTEXT: \"{context_text}\"\n",
    "    ---\n",
    "    Based on the image at the provided URL and its caption, describe the primary scientific finding in one clear sentence.\n",
    "    This sentence will be used to create knowledge graph relationships, so be precise and factual.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the message payload with both text and image\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": vision_prompt_text},\n",
    "            {\"type\": \"image_url\", \"image_url\": image_url},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the vision model\n",
    "    response = llm_vision.invoke([message])\n",
    "    finding_text = response.content\n",
    "\n",
    "    # Now, process this description to extract graph elements\n",
    "    if finding_text:\n",
    "        document = Document(page_content=finding_text)\n",
    "        graph_documents = transformer.convert_to_graph_documents([document])\n",
    "        graph.add_graph_documents(graph_documents, base=paper_node)\n",
    "\n",
    "        # Create the VisualEvidence node for the image\n",
    "        for doc in graph_documents:\n",
    "            for node in doc.nodes:\n",
    "                 # We only want to link to non-Paper nodes\n",
    "                if node.label != 'Paper':\n",
    "                    cypher_query = \"\"\"\n",
    "                    MERGE (p:Paper {id: $paper_id})\n",
    "                    MERGE (v:VisualEvidence {id: $image_id, type: 'Image', content: $content, caption: $caption})\n",
    "                    MERGE (c:%s {id: $concept_id})\n",
    "                    MERGE (v)-[:ILLUSTRATES]->(c)\n",
    "                    MERGE (p)-[:HAS_EVIDENCE]->(v) // Link the paper to the visual\n",
    "                    \"\"\" % node.label\n",
    "                    graph.query(cypher_query, params={\n",
    "                        \"paper_id\": paper_node['properties']['id'],\n",
    "                        \"image_id\": image_id,\n",
    "                        \"content\": image_url,\n",
    "                        \"caption\": context_text,\n",
    "                        \"concept_id\": node.id\n",
    "                    })\n",
    "\n",
    "print(\"Processing functions defined with actual vision model implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e6ca4",
   "metadata": {},
   "source": [
    "### Main Ingestion Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adec8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing Loop ---\n",
    "\n",
    "# 1. Load and Pre-process Supporting Data for Robust Matching\n",
    "try:\n",
    "    with open(IMAGES_FILE, 'r') as f:\n",
    "        image_url_map = json.load(f)\n",
    "    \n",
    "    # Load the CSV\n",
    "    papers_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # --- IMPROVEMENT: Create a direct mapping from PMC ID to Title and URL ---\n",
    "    # Extract PMC ID from the URL link. Assumes format like \".../PMC12345/\"\n",
    "    papers_df['pmc_id'] = papers_df['Link'].str.extract(r'(PMC\\d+)', expand=False)\n",
    "    \n",
    "    # Drop any rows where a PMC ID couldn't be extracted\n",
    "    papers_df.dropna(subset=['pmc_id'], inplace=True)\n",
    "    \n",
    "    # Create dictionaries for fast, reliable lookup using the PMC ID as the key\n",
    "    id_to_title_map = pd.Series(papers_df.Title.values, index=papers_df.pmc_id).to_dict()\n",
    "    id_to_url_map = pd.Series(papers_df.Link.values, index=papers_df.pmc_id).to_dict()\n",
    "    \n",
    "    print(\"Supporting data loaded and pre-processed for direct PMC ID matching.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not load or process supporting data files: {e}\")\n",
    "    # Exit or handle error appropriately\n",
    "\n",
    "# 2. Get list of text files to process\n",
    "text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith('.txt')]\n",
    "\n",
    "# Regex to find all our placeholders for tables and images\n",
    "media_pattern = re.compile(r'(img-[a-zA-Z0-9]+|table\\d+)')\n",
    "\n",
    "# 3. Iterate through each paper file\n",
    "for filename in text_files:\n",
    "    # --- IMPROVEMENT: This section is now much cleaner and more reliable ---\n",
    "    paper_id = os.path.splitext(filename)[0] # e.g., \"PMC12345\"\n",
    "    print(f\"\\n--- Processing Paper: {paper_id} ---\")\n",
    "    \n",
    "    # Use the PMC ID to directly and safely look up the title and URL\n",
    "    # The .get() method returns 'Unknown Title' or 'URL not found' if the ID isn't in our map\n",
    "    paper_title = id_to_title_map.get(paper_id, \"Title Not Found in CSV\")\n",
    "    paper_url = id_to_url_map.get(paper_id, \"URL Not Found in CSV\")\n",
    "\n",
    "    if \"Not Found\" in paper_title:\n",
    "        print(f\"  [WARN] Metadata for {paper_id} not found in {CSV_FILE}. Skipping metadata creation.\")\n",
    "        continue # Or handle as you see fit\n",
    "\n",
    "    # Create the central Paper node for this document\n",
    "    graph.query(\n",
    "        \"MERGE (p:Paper {id: $id}) SET p.title = $title, p.url = $url\",\n",
    "        params={\"id\": paper_id, \"title\": paper_title, \"url\": paper_url}\n",
    "    )\n",
    "    paper_node = {\"type\": \"Paper\", \"properties\": {\"id\": paper_id}}\n",
    "\n",
    "\n",
    "    # --- The rest of the content processing logic remains the same ---\n",
    "    file_path = os.path.join(TEXT_FOLDER, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        full_text = f.read()\n",
    "\n",
    "    last_end = 0\n",
    "    for match in media_pattern.finditer(full_text):\n",
    "        start, end = match.span()\n",
    "        media_id = match.group(0)\n",
    "\n",
    "        # Process the text chunk before this media item\n",
    "        text_chunk = full_text[last_end:start]\n",
    "        process_text_chunk(text_chunk, paper_node)\n",
    "\n",
    "        # Process the media item itself\n",
    "        # Grab ~300 chars of context around the ID for the caption\n",
    "        context_start = max(0, start - 150)\n",
    "        context_end = min(len(full_text), end + 150)\n",
    "        context_text = full_text[context_start:context_end]\n",
    "\n",
    "        if media_id.startswith('table'):\n",
    "            process_table(media_id, context_text, paper_node)\n",
    "        elif media_id.startswith('img-'):\n",
    "            process_image(media_id, context_text, paper_node, image_url_map)\n",
    "        \n",
    "        last_end = end\n",
    "\n",
    "    # Process any remaining text after the last media item\n",
    "    remaining_text = full_text[last_end:]\n",
    "    process_text_chunk(remaining_text, paper_node)\n",
    "\n",
    "print(\"\\n--- Ingestion Complete! ---\")\n",
    "print(\"Your Neo4j database is now populated with the knowledge graph.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
